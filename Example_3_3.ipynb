{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bound_3dim(payoff,\n",
    "                       generate_samples_mu1,\n",
    "                       generate_samples_mu2,\n",
    "                       generate_samples_mu3,\n",
    "                        dynamic_options = True,\n",
    "                        lower_bound=True,\n",
    "                        batch_size=2**8,\n",
    "                        gamma = 100,\n",
    "                        max_magnitude = 1000,\n",
    "                        max_iter = 1000,\n",
    "                        nr_neurons = 128,\n",
    "                        l_r = 0.0001,\n",
    "                        option_strike1 = 98,\n",
    "                        option_strike2 = 98,\n",
    "                        depth = 3,\n",
    "                        dynamic_factor = 1,\n",
    "                        lower_epsilon = 0,\n",
    "                        upper_epsilon = 0):\n",
    "    # Create Tensors for the Input\n",
    "    def build_model(dynamic_options):\n",
    "        x_1 = keras.Input(shape=(1,),name = \"x_1\")\n",
    "        x_2 = keras.Input(shape=(1,),name = \"x_2\")\n",
    "        x_3 = keras.Input(shape=(1,),name = \"x_3\")\n",
    "        x_12 = layers.concatenate([x_1, x_2])\n",
    "        \n",
    "        # Create the NN       \n",
    "        u_1 = layers.Dense(nr_neurons,activation = \"relu\")(x_1)\n",
    "        u_2 = layers.Dense(nr_neurons,activation = \"relu\")(x_2)\n",
    "        u_3 = layers.Dense(nr_neurons,activation = \"relu\")(x_3)\n",
    "        H_1 = layers.Dense(nr_neurons,activation = \"relu\")(x_1)\n",
    "        H_2 = layers.Dense(nr_neurons,activation = \"relu\")(x_12)\n",
    "        if dynamic_options:\n",
    "            H_121 = layers.Dense(nr_neurons,activation = \"relu\")(x_1)\n",
    "            H_131 = layers.Dense(nr_neurons,activation = \"relu\")(x_1)\n",
    "            H_231 = layers.Dense(nr_neurons,activation = \"relu\")(x_12)\n",
    "        \n",
    "        # Create deep layers\n",
    "        for i in range(depth):\n",
    "            u_1 = layers.Dense(nr_neurons,activation = \"relu\")(u_1)\n",
    "            u_2 = layers.Dense(nr_neurons,activation = \"relu\")(u_2)\n",
    "            u_3 = layers.Dense(nr_neurons,activation = \"relu\")(u_3)\n",
    "            H_1 = layers.Dense(nr_neurons,activation = \"relu\")(H_1)            \n",
    "            H_2 = layers.Dense(nr_neurons,activation = \"relu\")(H_2)\n",
    "            if dynamic_options:\n",
    "                H_121 = layers.Dense(nr_neurons,activation = \"relu\")(H_121)\n",
    "                H_131 = layers.Dense(nr_neurons,activation = \"relu\")(H_131)\n",
    "                H_231 = layers.Dense(nr_neurons,activation = \"relu\")(H_231)\n",
    "          \n",
    "        # Output Layers\n",
    "        u_1_out = layers.Dense(1,name = \"u_1\")(u_1)\n",
    "        u_2_out = layers.Dense(1,name = \"u_2\")(u_2)\n",
    "        u_3_out = layers.Dense(1,name = \"u_3\")(u_3)\n",
    "        H_1_out = layers.Dense(1,name = \"H_1\")(H_1)\n",
    "        H_2_out = layers.Dense(1,name = \"H_2\")(H_2)\n",
    "        if dynamic_options:\n",
    "            H_121_out = layers.Dense(1,name = \"H_121\")(H_121)\n",
    "            H_131_out = layers.Dense(1,name = \"H_131\")(H_131)\n",
    "            H_231_out = layers.Dense(1,name = \"H_231\")(H_231)        \n",
    "        # Build the model\n",
    "            model = keras.Model(inputs=[x_1,x_2,x_3],\n",
    "                                outputs = [u_1_out,u_2_out,u_3_out,H_1_out,H_2_out,\n",
    "                                           H_121_out,H_131_out,H_231_out])\n",
    "        else:\n",
    "            model = keras.Model(inputs=[x_1,x_2,x_3],\n",
    "                    outputs = [u_1_out,u_2_out,u_3_out,H_1_out,H_2_out])\n",
    "        return model\n",
    "    \n",
    "    # Loss function\n",
    "    def loss(model,x_1,x_2,x_3,epoch):\n",
    "        u_1, u_2, u_3, H_1, H_2 = model({\"x_1\":x_1,\"x_2\":x_2,\"x_3\":x_3})\n",
    "        hedge = u_1 + u_2 + u_3 + H_1*(x_2-x_1) + H_2*(x_3-x_2)\n",
    "        s = payoff(x_1,x_2,x_3)*(1-2*int(lower_bound)) - hedge\n",
    "        loss = tf.reduce_mean(u_1+u_2+u_3)+ \\\n",
    "        0.5*gamma_dynamic(gamma, epoch)*tf.reduce_mean(tf.pow(tf.nn.relu(s),2))\n",
    "        return loss\n",
    "    \n",
    "    def loss_dynamic(model,x_1,x_2,x_3,p_121,p_131,p_231,epoch):\n",
    "        u_1, u_2, u_3, H_1, H_2, H_121, H_131, H_231 = model({\"x_1\":x_1,\"x_2\":x_2,\"x_3\":x_3})\n",
    "        hedge = u_1 + u_2 + u_3 + H_1*(x_2-x_1) + H_2*(x_3-x_2)\n",
    "        hedge = hedge + H_121*(tf.nn.relu(x_2-option_strike1)-p_121)+ \\\n",
    "                        H_131*(tf.nn.relu(x_3-option_strike2)-p_131)+\\\n",
    "                        H_231*(tf.nn.relu(x_3-option_strike2)-p_231)\n",
    "        s = payoff(x_1,x_2,x_3)*(1-2*int(lower_bound)) - hedge\n",
    "        loss = tf.reduce_mean(u_1+u_2+u_3)+ \\\n",
    "        0.5*gamma_dynamic(gamma, epoch)*tf.reduce_mean(tf.pow(tf.nn.relu(s),2))\n",
    "        return loss\n",
    "    \n",
    "    # Define Gradient    \n",
    "    def grad(model,x_1,x_2,x_3,epoch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = loss(model,x_1,x_2,x_3,epoch)\n",
    "        return loss_value, tape.gradient(loss_value,model.trainable_variables)\n",
    "\n",
    "    def grad_dynamic(model,x_1,x_2,x_3,p_121,p_131,p_231,epoch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = loss_dynamic(model,x_1,x_2,x_3,p_121,p_131,p_231,epoch)\n",
    "        return loss_value, tape.gradient(loss_value,model.trainable_variables)\n",
    "\n",
    "    # Dynamic Batch Size, Increase over Time\n",
    "    def batch_size_dynamic(batch_size, epoch):\n",
    "        return round(((max_iter-epoch)/max_iter)*(batch_size*dynamic_factor)+(epoch/max_iter)*(batch_size))\n",
    "    \n",
    "    # Dynamic Gamma, Increase over Time    \n",
    "    def gamma_dynamic(gamma, epoch):\n",
    "        return (((max_iter-epoch)/max_iter)*(gamma*dynamic_factor)+(epoch/max_iter)* (gamma))\n",
    "    \n",
    "    # Create Optimizer and Model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = l_r, beta_1=0.9, beta_2=0.999)\n",
    "    model = build_model(dynamic_options)\n",
    "    losses = []\n",
    "\n",
    "    if dynamic_options:\n",
    "        # Training Loop\n",
    "        for epoch in range(int(max_iter)):\n",
    "            x_1 = next(generate_samples_mu1(batch_size_dynamic(batch_size,epoch)))\n",
    "            x_2 = next(generate_samples_mu2(batch_size_dynamic(batch_size,epoch)))\n",
    "            x_3 = next(generate_samples_mu3(batch_size_dynamic(batch_size,epoch)))\n",
    "            p_121 = tf.random.uniform(shape = [int(batch_size_dynamic(batch_size,epoch)),1])*tf.nn.relu(x_1-upper_epsilon-lower_epsilon-tf.nn.relu(x_1-option_strike1))+lower_epsilon+tf.nn.relu(x_1-option_strike1)\n",
    "            p_131 = tf.random.uniform(shape = [int(batch_size_dynamic(batch_size,epoch)),1])*tf.nn.relu(x_1-upper_epsilon-lower_epsilon-tf.nn.relu(x_1-option_strike2))+lower_epsilon+tf.nn.relu(x_1-option_strike2)\n",
    "            p_231 = tf.random.uniform(shape = [int(batch_size_dynamic(batch_size,epoch)),1])*tf.nn.relu(x_2-upper_epsilon-lower_epsilon-tf.nn.relu(x_2-option_strike2))+lower_epsilon+tf.nn.relu(x_2-option_strike2)\n",
    "            loss_value, grads = grad_dynamic(model, x_1,x_2,x_3,p_121,p_131,p_231, epoch)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            losses.append(loss_value.numpy()*(1-2*int(lower_bound)))\n",
    "            if epoch % 100 == 0 and epoch > 0:\n",
    "                print(\"Iteration:{}, Avg. Loss: {}\".format((epoch),np.mean(losses[-(round(epoch*0.05))])))\n",
    "    else:\n",
    "        # Training Loop\n",
    "        for epoch in range(int(max_iter)):\n",
    "            x_1 = next(generate_samples_mu1(batch_size_dynamic(batch_size,epoch)))\n",
    "            x_2 = next(generate_samples_mu2(batch_size_dynamic(batch_size,epoch)))\n",
    "            x_3 = next(generate_samples_mu3(batch_size_dynamic(batch_size,epoch)))\n",
    "            loss_value, grads = grad(model, x_1,x_2,x_3,epoch)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            losses.append(loss_value.numpy()*(1-2*int(lower_bound)))\n",
    "            if epoch % 100 == 0 and epoch > 0:\n",
    "                print(\"Iteration:{}, Avg. Loss: {}\".format((epoch),np.mean(losses[-(round(epoch*0.05))])))\n",
    "      \n",
    "            \n",
    "    print(\"Iteration result: {}\".format(np.mean(losses[-(round(max_iter*0.05))])))\n",
    "    return np.mean(losses[-(round(max_iter*0.05))]), model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example with lognormal marginals\n",
    "Compute the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_1 = 1\n",
    "t_2 = 2\n",
    "t_3 = 3\n",
    "vol = 0.1\n",
    "S_0 = 1\n",
    "\n",
    "\n",
    "def log_norm1(batch_size):\n",
    "    while True:\n",
    "        x = S_0*tf.math.exp(np.sqrt(vol*t_1)*tf.random.normal(shape = [int(batch_size),1])-vol*(t_1/2))\n",
    "        yield x\n",
    "            \n",
    "def log_norm2(batch_size):\n",
    "    while True:\n",
    "        x = S_0*tf.math.exp(np.sqrt(vol*t_2)*tf.random.normal(shape = [int(batch_size),1])-vol*(t_2/2))\n",
    "        yield x\n",
    "        \n",
    "def log_norm3(batch_size):\n",
    "    while True:\n",
    "        x = S_0*tf.math.exp(np.sqrt(vol*t_3)*tf.random.normal(shape = [int(batch_size),1])-vol*(t_3/2))\n",
    "        yield x\n",
    "\n",
    "def payoff(x,y,z):\n",
    "   return tf.nn.relu((1/3)*(x+y+z)-S_0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the grid\n",
    "epsilon_lower_list  = [e*S_0 for e in [0,0.005,0.01,0.015,0.02]]\n",
    "epsilon_upper_list = [e*S_0 for e in [0.65,0.7,0.75,0.8,0.85]]\n",
    "\n",
    "grid = np.zeros([len(epsilon_lower_list),len(epsilon_upper_list)])\n",
    "\n",
    "for i in range(len(epsilon_lower_list)):\n",
    "    for j in range(len(epsilon_upper_list)):\n",
    "        grid[i,j] = compute_bound_3dim(payoff,\n",
    "                        log_norm1,\n",
    "                        log_norm2,\n",
    "                        log_norm3,\n",
    "                        dynamic_options = True,\n",
    "                        lower_bound= False,\n",
    "                          batch_size=2**(13),\n",
    "                          gamma = 10000,\n",
    "                          max_magnitude = 1000,\n",
    "                          max_iter = 50000,\n",
    "                          nr_neurons = 64*2,\n",
    "                          depth = 5,\n",
    "                          l_r =   0.0001,\n",
    "                          dynamic_factor = 1,\n",
    "                          option_strike1 = 1,\n",
    "                          option_strike2 = 1,\n",
    "                          lower_epsilon = epsilon_lower_list[i],\n",
    "                          upper_epsilon = epsilon_upper_list[j])[0]\n",
    "        \n",
    "grid_df = pd.DataFrame(grid)\n",
    "grid_df.to_csv(\"./grid.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the result. (Here only for 1 simulation, 30 take a lot of time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = pd.read_csv(\"grid.csv\") # Here we can also combine more than 1 simulation of grids. In the paper: 30\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "x1 = np.array([0,0.005,0.01,0.015,0.02])*100\n",
    "#x1 = list(reversed(x1))\n",
    "y1 = np.array([0.65,0.7,0.75,0.8,0.85])*100\n",
    "#y1 = list(reversed(y1))\n",
    "bound_wo = 0.17660294473171234\n",
    "X, Y = np.meshgrid(x1, y1)\n",
    "ax.plot_wireframe(X, Y , np.ones((len(x1),len(y1)))*np.max(np.max(average)),color = \"red\",alpha=1)\n",
    "ax.plot_surface(X, Y , average,color = \"cornflowerblue\")\n",
    "ax.plot_wireframe(X, Y , average,color = \"black\")\n",
    "ax.view_init(35,  35)\n",
    "ax.xaxis.set_rotate_label(False)\n",
    "ax.yaxis.set_rotate_label(False)\n",
    "ax.zaxis.set_rotate_label(False)\n",
    "\n",
    "ax.set_xlabel(r'$\\varepsilon_1$', fontsize=25)\n",
    "ax.set_ylabel(r'$\\varepsilon_2$', fontsize=25)\n",
    "ax.set_zlabel(r'$P_{\\Xi_{({p}_{i,j,k}+\\varepsilon_1,\\overline{p}_{i,j,k}-\\varepsilon_2)}}(\\Phi)$',\n",
    "              fontsize=25,rotation = 90)\n",
    "ax.set_xticks(np.arange(0,2.25,0.25))\n",
    "ax.set_yticks(np.arange(65,87.5,2.5))\n",
    "ax.set_zticks([14.5,15,15.5,16,16.5,17])\n",
    "ax.set_xticklabels([0,0.25,0.5,0.75,1,1.25,1.5,175,2], fontsize=10)\n",
    "ax.set_yticklabels([65,67.5,70,72.5,75,77.7,80,82.5,85], fontsize=10)\n",
    "ax.set_zticklabels([14.5,15,15.5,16,16.5,17], fontsize=10)\n",
    "plt.savefig('3d_improved_bounds.eps', format='eps')\n",
    "plt.show()a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
